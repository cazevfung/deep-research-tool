{
  "success": true,
  "bv_id": "BV1ThJyzzEPZ",
  "url": "https://www.bilibili.com/video/BV1ThJyzzEPZ/",
  "content": "你一定有过这样的经历，当我们在AI聊天机器人中问出一个问题的时候，AI不仅会给出我们答案，还会逻辑清晰的给出他每一步的思考过程。那一刻仿佛和我们对话的根本就不是一个模型，那分明就是一个会思考、能分析，甚至还能揣度我们用意的某种智能的存在。那你是否也遇到过另一种情况，A的推理明明逻辑严谨头头是道，但在最终却信心十足的给出一个和推理完全相反的结论。比如说曾经有研究者问J奈, 1776年是平年还是闰年，大模型的思维过程是这样的，1776年可以被四整除，但它又不是世纪年，所以给出的答案是一个平年，但是可以被四整除，又不是世纪年的年份，其实应该是一个闰年，大模型在思考时正确的使用了闰年的计算方式，但得出的结论却是错误的，为什么会这样呢？一篇来自美国亚利桑那大学名为斯维链是否是大语言模型的幻象的论文，为我们提供了一个全新的视角。今天我们抛弃论文中所有的公式计算，让老王用普通人也可以听得懂的语言解释一下这篇论文到底讲了什么东西。这个论文的主要观点简单来说就是所谓的A思维链，并不是我们所理解的抽象推理的能力，它只是一种高度依赖于训练数据的模式匹配。也就是说AI不是在真正的思考，而是在他的记忆中找到无数看起来像是思考的片段。然后根据我们的问题，把这些片段以一种概率上最合理的方式连接起来而已，从而生成一段看起来逻辑通顺的回答，回到我们最开始的1776年是平年还是闰年的例子。大模型的思考过程是1776年可以被四整除，但又不是世纪年，所以1776年是一个平年，但这个结论其实并不是根据思考而来的，它的实际来源是大模型在训练过程中见过各种平年、闰年计算方法的文字片段。这些片段对应的就是大模型的思考部分，而这些文字片段之后，往往后面紧跟着的例子都是一个计算平年的例子，所以大模型也就跟着输出了1776年是平年的结论。在这个过程中，大模型内部其实根本没有用1776这个数字进行过计算，它得出这个结论只是因为训练的语料中计算平闰年算法后紧跟着平年的例子比较多而已。好了，以上就是研究者的合理猜想，那它怎么证明它是正确的呢，毕竟大模型的内部参数应该怎么解释，至今科学界都还没搞明白，想要进行完整的证明暂时是不可能的了，所以研究者设计了一个非常有趣的实验叫做data，从侧面证明了他们的观点。在这个实验中，研究人员从零开始训练了一个语言模型。这个模型能做的事情非常非常的简单，它只支持两种操作。一种操作是字母加密规则，就是将字符串中的每一个字母在字母表上向后移动13位，比如说abcd就变成了noq。第二种操作是循环位移，也就是将字符串的最后一个字母挪到最前面，比如说abc d就变成了dc然后就能再变成cd b我们训练的目标不仅是让AI给出最终的答案，还要展示变换的过程。比如说Abd先字母加密再循环位移，AI不应该直接输出答案Q N O P，而是应该先输出推理过程，也就是思维链A D经过字母加密得到N O P，Q N O Q再经过循环位移得到Q N O P，所以结果是Q N O P。这个实验的精妙之处在于，在训练模型的时候，所有的训练数据都是研究人员自己生成的，于是在训练之后，人们可以精确的控制A收到的问题是见过的还是没见过的？如果是没见过的问题，那又是怎么没见过呢？从而研究人员可以像控制实验变量一样，来引入各种没见过的情况。下面我们就来看看研究人员都提供了什么样的没见过的数据，以及他们为什么能从侧面反映思维链只示模式匹配。作者在文章中设计了三个实验，任务泛化、长度泛化和格式泛化。我们一个一个讲，在任务泛化的实验中，研究人员只用字母加密训练了模型，直到这个模型能够100%的解决所有的字母加密问题。然后研究人员突然用一个循环位移的问题来观察模型的思维链。如果说模型可以做到知识的泛化，研究人员期待的结果是要么模型可以识别出这些操作都是字母的变换，所以通过思维链推导出循环位移的新规则。要么退一步就直接承认不知道该怎么做，但结果却是模型会固执的想把字母加密的规则套用在循环位移的问题上。之后，研究人员又对模型进行了微调，他们只新增了不到0.02%的关于循环位移的数据，就让模型迅速的学会了处理这个问题。这个实验证明模型在处理字母加密时，并没有理解字母加密背后隐藏着的位移13位的算法，所以在遇到没有见过的循环位移的问题时，不能在推断出新的算法，也不能判断已有的算法无法处理新的问题，而少量的微调则弥补上了这个没有见过的模式。不仅如此，研究人员还尝试在测试中加入训练时没有见过的组合，比如说训练的数据永远是先字母加你再循环位移，测试的时候却要求AI先循环位移，然后再字幕加密，这次的结果更加有趣。模型输出的推理过程和问题是无关的，但是结果却又是正确的，你能猜到这是为什么吗？想到的同学请留言告诉我。这些证据都间接的说明了模型并没有理解训练数据背后所隐藏的算法。在这个实验中，研究人员通过给模型提出没有见过的问题泛化了任务。而在第二个实验中，研究人员则尝试泛化长度。这一次我们只用两步推理来训练模型。比如说连续两次字母加密，连续两次循环位移，或者字母加密加长循环位移，总之训练数据都是两步的。然后在推理的时候，我们让AI只进行一步推理，比如说只字母加密，或者进行三步推理。比如说连续三次进行加密，结果输出的思维链出现了明显的问题，面对一步的问题时，AI常常强行编造出第二步，而面对一个三步的问题时，AI往往推理了两步就停止了。这表明模型的思考过程并不是按照问题的实际需求所生成的，更像是在填充一个固定长度的模板。第三个实验是格式泛化，这是最能体现AI只是在做模式匹配的一个实验了。实验人员在训练AI的时候，让他只看到特定的格式的指令，比如说problem冒号Abd中括号加密，但是在测试的时候，研究人员仅仅是把problem替换成了question，或者把中括号替换成了小括号，就导致了模型性能的显著下降。而真正的逻辑推理应该是抽象于符号与语法的，但是模型却对这些表面形式上的改动如此的敏感，恰恰证明了它所依赖的并非深层次的逻辑，而仅仅是对文本表面模式的复现。也许你会质疑，这可能只是因为研究人员训练的模型太小了，如果说换成gp 5这种巨无霸，结果会不会不同呢？原文的作者也讨论了这个问题，他们用不同大小的模型重复的进行了实验，还调整了其他的参数。结论是这种依赖训练数据难以泛化的问题依然是存在的，这说明问题不出在模型不够大的上面，而出在他们学习的方式上面。最后作者说这些实验并非为了贬低A而是让我们以一种更成熟的方式去使用它。首先，我们要保持健康的怀疑，永远不要把AI输出的内容当做绝对的真理。因为AI是非常擅长用不容置疑的语气来包装一个完全错误的结论的。然后我们最好要主动测试AI的边界，我们不妨可以设计一些超出常规的问题，这样可以更好的让我们把握AI的边界。最后一点也是最最重要的，我们自己才是真正的思考者。AI只是我们思考的辅助工具，而不是代替者。我们之所以对会思考的AI如此的着迷，或许是源于我们对创造同类的渴望。我们实在是太想看到一个会思考的机器可以陪我们喜怒哀乐的机器了，以至于我们不自觉的把流畅的表达等同于深刻的思考。这篇论文与其说是揭露了A的缺陷，不如说是修正了我们的认识。也许通往人工智能的路上，重要的并非是让A学会像人一样思考，而是我们人类学会如何善用一个和我们思维方式完全不同的异类。这里是同学老王，我们下期再见。",
  "title": "",
  "author": "",
  "publish_date": "",
  "source": "Bilibili (via SnapAny + Paraformer)",
  "language": "zh-CN",
  "word_count": 3017,
  "extraction_method": "snapany_paraformer",
  "extraction_timestamp": "2025-11-13T12:00:36.560789",
  "batch_id": "20251113_034911",
  "link_id": "bili_req17",
  "error": null,
  "summary": {
    "transcript_summary": {
      "key_facts": [
        "AI在回答问题时会展示思维链过程，看似逻辑清晰",
        "1776年可被4整除且非世纪年，应为闰年但模型错误判断为平年",
        "论文《斯维链是否是大语言模型的幻象》提出思维链为模式匹配而非真实推理",
        "思维链是模型从训练数据中复现的文本片段组合，非自主计算",
        "模型在训练中未真正理解算法，仅依赖语料中出现的模式",
        "研究人员设计了从零训练的语言模型，仅支持字母加密和循环位移操作",
        "模型需输出推理过程，如先加密再位移，不能直接给出答案",
        "实验通过控制问题是否见过来测试模型泛化能力",
        "任务泛化实验中模型无法处理未见过的循环位移操作",
        "微调仅增加0.02%新数据即让模型学会处理新任务",
        "模型在未见组合任务中输出与问题无关的推理但结果正确",
        "长度泛化实验中模型对一步或三步问题强行填充固定模板",
        "格式泛化实验中更换指令符号导致性能显著下降",
        "模型对表面格式变化敏感，说明其依赖文本模式而非深层逻辑",
        "不同规模模型重复实验均显示相同泛化缺陷"
      ],
      "key_opinions": [
        "思维链并非真正的抽象推理，而是高度依赖训练数据的模式匹配",
        "模型内部并未真正进行数学计算，只是复现语料中的表达序列",
        "AI擅长用不容置疑的语气包装错误结论，应保持怀疑态度",
        "人类不应将AI视为思考替代者，而应作为辅助工具使用",
        "我们对会思考的AI着迷，源于对创造同类的渴望",
        "流畅表达不等于深刻思考，需警惕认知错觉",
        "论文目的不是贬低AI，而是修正人类对AI的认知",
        "通往AI的道路关键在于人类学会善用异类思维方式",
        "当前模型的学习方式存在根本性局限，非规模问题",
        "设计超出常规的问题有助于测试AI边界"
      ],
      "key_datapoints": [
        "训练数据中新增不到0.02%的循环位移样本即可使模型学会新任务",
        "任务泛化实验中模型在未见任务上表现失败，无泛化能力",
        "长度泛化实验中模型对一步问题强行添加第二步推理",
        "三步问题中模型常只推理两步即停止，未完成任务",
        "格式泛化实验中仅更改指令符号（如problem→question）导致性能下降",
        "所有实验在不同规模模型上重复验证，结果一致",
        "模型在未见组合任务中输出正确结果但推理过程无关",
        "训练数据均为两步推理，测试中引入一步或三步任务",
        "模型在测试中对新格式的响应准确率显著降低",
        "研究者通过自动生成数据实现完全可控的实验环境"
      ],
      "topic_areas": [
        "思维链本质",
        "模式匹配机制",
        "模型泛化能力",
        "训练数据依赖",
        "格式敏感性",
        "推理真实性",
        "AI边界测试",
        "人类认知偏差",
        "AI辅助角色",
        "语言模型局限"
      ],
      "word_count": 20,
      "total_markers": 35
    },
    "comments_summary": {},
    "created_at": "2025-11-13T12:06:13.987627",
    "model_used": "qwen-flash"
  }
}