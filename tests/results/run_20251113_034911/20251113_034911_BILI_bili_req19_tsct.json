{
  "success": true,
  "bv_id": "BV168j7zCE6D",
  "url": "https://www.bilibili.com/video/BV168j7zCE6D/",
  "content": "Rug是为了解决当上下文太长的时候，大语言模型容易因为抓不住重点，开始胡说八道这个问题而产生的架构。它的基本思路是这样的，先把文章拆成很多的小段，然后对每一段做embedding，然后再把所有的embedding存进一个向量数据库。然后用户提问的时候，再从数据库里面找出那些和问题语义接近的片段，一起发给大语言模型。如果你想先了解一下rug的基本原理，可以先去看看我的这一期视频，十分钟把rug的核心概念讲的明明白白，连接我会放在视频简介里面。这次我们就来一起从零开始来实现一套rug架构。为了防止AI偷偷用它自己的知识库，今天我们就选用一篇它肯定没见过的旷世奇闻。关于令狐冲转生为史莱姆并对世界献上了美好的言报这件事，文章讲述了令狐冲异世界转生为史莱姆，并且凭借着前世的记忆，成功的悟出了岩豹术一路上扮猪吃虎的励志故事。有兴趣的小伙伴可以自行的阅读一下连接，我会放在视频简介之中。好，首先我们的第一步是要把文章拆成许多的小片段。其实这篇文章写的还挺规整的，因为每个段落之间都有两个回车来做分割，而且每一个段落之间呢它们的字数也是比较均匀的。所以这一次我们就直接用两个回车符来作为切分的依据。代码我们就写到ch点P Y文件中，其中我已经预先写好了一个函数，叫做read data。这个函数会把刚才的文章读取为一个完整的字符串。现在我们来增加一个分块函数，叫做get chunk。这个函数的返回值呢就是分块后的文章，是一个字符串的列表。然后就像我们刚才说的那样，我们按照两个回车符来对文章进行分割。好，到这里我们第一版的分块函数就写完了。现在我们来写一个慢函数，运行一下看看效果。可以看到我们的分块结果其实还是不错的。但是这里有一个小问题，就是这些章节的标题自己也被切分成了一个段落。但是它们太短了，把它们单独当做一个段落不太合适。所以呢我们这里来稍微修改一下逻辑。如果说这一段是以井号开头的那我们就把它和后面的正文合并在一起。好，现在来看我的具体实现。这个代码的逻辑我就不多解释了，都是比较简单的字符串处理。现在我们再来运行一下，看一下效果。可以看到我们的标题已经和正文连接在一起了，我觉得现在的效果就挺不错的，咱们的分块函数也就差不多了。当然了，除了手写分块函数，网上也有不少现成的分块算法。比如说lan里面的recursive character text splitter就是一个非常强大的分块算法，有兴趣的朋友可以自己研究一下。我们这里只是做基本的展示，就不引入额外的复杂度了。好，现在我们已经完成了对文章的分块。分块之后第二步我们需要对每一个分块来做embedding，然后存进一个向量的数据库。代码呢我们就写到embed点外文件中，我这边选的向量数据库是chma db因为它用起来比较简单。然后embedding模型我选择的是Google的embedding模型，我们先来安装一下依赖。因为这里我使用了Google的embedding模型，所以我们还需要一个额外的api key，它需要被放到一个叫做Google api的环境变量里面。我这边已经提前的配置好了，我打印出来看一下，接着我们就来写一个embedding函数，函数的名字我们就叫embed。这个函数接收一段文字来当做参数，然后返回这段文字对应的embedding，也就是一个浮点数的数组。我们调用J mini embedding模型的接口大概是这个样子的。其中的model参数是embedding模型的名称，这里我使用的模型是mini embedding ex p 0307。Content参数就是我们需要embedding的文字内容了。但是Google的embedding模型有一点特别，它把embedding分成了两个类别，一类是用来存储的，一类是用来查询的。为什么要这么搞呢？我们来举一个例子，比如说你有两段文本，老王爱吃瓜和老王喜欢吃瓜，这两句的意思呢差不多，所以他们embedding的位置就会比较接近。但是如果你之后的问题是老王喜欢吃什么？这句话虽然在语义上跟前两句是相关的，但是他们之间的形式差别还是挺大的。他们的embedding距离可能就没有那么近了。所以Google的做法是在做embedding的时候，你要明确的告诉他这段文本的用途是什么，是用来存储的还是用来查询的。也就是说我们在存老王爱吃瓜和老王喜欢吃瓜的时候，我们需要用存储模式。而在查询老王喜欢吃什么的时候，我们需要用查询模式。然后就能非常神奇的把这两个看起来差的比较远的句子，在embedding的向量空间里面拉的很近。它具体是怎么实现的，谷歌并没有说，总之我们按它的规定来就行了。这里需要注意的是，除了Google，其他大部分的embedding模型是没有这个存储和查询之分的，他们用的是相同的接口。所以这里我们来给embedding函数增加一个参数，用来表示这次embedding是用来存的还是用来查的。接着我们通过config参数把这个信息传递给模型。Task type参数在存储的时候取值是retrieval document，而在查询的时候取值是retrival ery，具体他们的值可以参考jaminite的官方文档，我会把链接放在视频简介里面。最后我们把bedding的结果直接返回回去。注意一下，在这里为了演示方便，我就直接使用了assert来判断返回值。而在实际的开发中，你需要做更稳妥的错误处理。然后我们再写个main函数测试一下。这里的get chunks函数就是我们刚刚写的分块函数，不过我的函数名好像写错了，我们来修正一下。Ok现在我们来打印出第一个chunk的embedding，看看是什么效果。好，我们来运行一下试试看。可以看到打印出的这一大串数字就是文章的第一个片段的embedding结果了。好，到了现在我们已经有了文章的分块，也有了embedding的方法。接下来我们要做的是生成一个chma db的实例，然后把每个文章分块的embedding和原文一对一的存到向量数据库里面。我们先来生成向量数据库的实例。在这段代码之中，我指定了数据库的存储位置，在当前目录的chroma点db文件夹下面。然后我还生成了一个数据表，数据表的名字呢我们可以随便的起，这里呢我用的是令狐冲的拼音。好，现在我们来写一个函数，依次对每一个chunk来做embedding。注意一下这里embed函数的store参数，我设置的是。因为我们现在是准备把结果存到数据库里面，接着我们使用chroma db的up方法，把embedding的结果存储到向量数据库之中。Chma要求我们为每一条数据都提供一个字符串类型的ID，这个ID其实没有太大的用处，这里我就直接用片段的索引当做ID了。而documents和embedding分别是原文和原文对应的embedding，最后我们在main函数中调用一下create db函数，把令狐冲的异世界之旅存储到数据库之中。好，现在我们来运行一下。",
  "title": "",
  "author": "",
  "publish_date": "",
  "source": "Bilibili (via SnapAny + Paraformer)",
  "language": "zh-CN",
  "word_count": 2874,
  "extraction_method": "snapany_paraformer",
  "extraction_timestamp": "2025-11-13T12:02:20.062680",
  "batch_id": "20251113_034911",
  "link_id": "bili_req19",
  "error": null,
  "summary": {
    "transcript_summary": {
      "key_facts": [
        "Rug架构用于解决大语言模型在长上下文场景下抓不住重点的问题",
        "Rug将文章拆分为多个小段落进行处理",
        "每个段落通过embedding技术转换为向量并存入向量数据库",
        "用户提问时，系统从数据库中检索语义相近的片段",
        "文章分块使用两个回车符作为分割依据",
        "标题段落若以井号开头则与后续正文合并",
        "使用ChromaDB作为向量数据库实现存储",
        "Google的embedding模型区分存储和查询两种模式",
        "embedding模型名称为mini embedding exp 0307",
        "embedding任务类型在存储时为retrieval document，查询时为retrieval query",
        "每个数据条目需提供字符串类型的ID，此处使用索引作为ID",
        "数据库实例创建于当前目录下的chroma.db文件夹中",
        "数据表名称为“linghu chong”（令狐冲拼音）"
      ],
      "key_opinions": [
        "使用两个回车符切分文章的方法在本文中效果良好",
        "标题与正文合并能避免短段落带来的信息碎片化问题",
        "Google的embedding模型在存储与查询模式上的设计具有独特优势",
        "该方法在演示中已达到预期效果，可作为基础实现参考",
        "引入复杂分块算法会增加不必要的开发复杂度"
      ],
      "key_datapoints": [
        "文章分块基于两个回车符进行分割",
        "embedding模型为mini embedding exp 0307",
        "存储任务类型值为retrieval document",
        "查询任务类型值为retrieval query",
        "数据库路径为当前目录下的chroma.db文件夹",
        "数据表名为“linghu chong”",
        "每条记录的ID为片段索引编号",
        "每个chunk的embedding返回为浮点数数组",
        "代码中使用assert进行结果验证（仅用于演示）",
        "未引入外部依赖库如langchain中的recursive character text splitter"
      ],
      "topic_areas": [
        "Rug架构原理",
        "文本分块策略",
        "Embedding模型应用",
        "向量数据库存储",
        "语义检索机制",
        "Google API集成",
        "代码实现流程",
        "任务类型区分"
      ],
      "word_count": 23,
      "total_markers": 28
    },
    "comments_summary": {},
    "created_at": "2025-11-13T12:06:30.715489",
    "model_used": "qwen-flash"
  }
}